---
layout: false
---

<script lang="ts">
  import CloudPlatformToggle from "$lib/components/docs/cloud-platform-toggle.svelte";
</script>

For each Gitpod installation, you need a **domain**. In this guide, we use `gitpod.example.com` as a placeholder for your domain. Gitpod also uses different subdomains for some components as well as dynamically for the running workspaces. That's why you need to configure your DNS server and your TLS certificates for your Gitpod domain with the following wildcards:

```
gitpod.example.com
*.gitpod.example.com
*.ws.gitpod.example.com
```

### Cluster ports

The entry point for all traffic is the `proxy` component which has a service of type `LoadBalancer` that allows inbound traffic on ports 80 (HTTP) and 443 (HTTPS) as well as port 22 (SSH access to the workspaces).

SSH access is required to work with desktop IDEs, such as [VS Code Desktop](/docs/references/ides-and-editors/vscode) and JetBrains via [JetBrains Gateway](/docs/integrations/jetbrains-gateway). To enable SSH, your **load balancer** needs to be capable of working with [L4 protocols](https://en.wikipedia.org/wiki/OSI_model#Layer_4:_Transport_layer).

<CloudPlatformToggle id="cloud-platform-toggle-networking">
<div slot="gcp">

In this guide, we use [load balancing through a standalone network endpoint group (NEG)](https://cloud.google.com/kubernetes-engine/docs/how-to/standalone-neg). For this, the Gitpod proxy service will get the following annotation by default:

```bash
cloud.google.com/neg: '{"exposed_ports": {"80":{},"443": {}}}'
```

For Gitpod, we support Calico as CNI only. You need to make sure that you DO NOT use [GKE Dataplan V2](https://cloud.google.com/kubernetes-engine/docs/concepts/dataplane-v2). That means, do not add the `--enable-dataplane-v2` flag during the cluster creation.

</div>
<div slot="aws">

It is suggested to create a dedicated VPC (and EKS instance) for Gitpod. `eksctl` can do this for you, but if VPCs have to be configured separately, follow `eksctl`’s [suggestions](https://eksctl.io/usage/vpc-configuration/).

> **Note**: You can also customize `eksctl`’s [vpc](https://eksctl.io/usage/vpc-subnet-settings/#custom-subnet-topology) creation to suit your existing configurations.

The VPC needs public and private subnets. All managed node groups and Gitpod services should run in the private subnet. Inbound access to the services should be through ALB/ELB services auto-provisioned by AWS based on the configuration used (standard LoadBalancer roles or creation of an Ingress). If running a jump host or VPN endpoint, it should be deployed in the public subnet.

> By default, when Gitpod is being installed, EKS will create a classic load balancer that you can point your DNS entries at. If you are unable to use a AWS Classic Load Balancer (e.g. because you use SSL certificates generated by AWS), please follow [the Setting up your EKS cluster with dual ALB + NLB load balancers guide](../advanced/eks-with-alb-and-nlb) _alongside_ this reference architecture guide.

If installing Calico, follow their [installation steps](https://projectcalico.docs.tigera.io/getting-started/kubernetes/managed-public-cloud/eks) and ensure you modify the `hostNetwork: True` option on the cert-manager installation options later.

</div>
<div slot="azure">

Azure automatically provisions [Azure public load balancers](https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard) that load balance public Gitpod services and provide public Internet connectivity for Gitpod's workloads. No additional configuration is required.

</div>

</CloudPlatformToggle>

### External DNS

You also need to configure your **DNS server**. If you have your own DNS server for your domain, make sure the domain with all wildcards points to your load balancer.

Creating a dedicated DNS zone is recommended when using cert-manager or external-dns but is not required. A pre-existing DNS zone may be used as long as the **cert-manager** and/or **external-dns** services are authorized to manage DNS records within that zone. If you are providing your own TLS certificates and will manually create A records pointing to Gitpod's public load balancer IP addresses then creating a zone is unnecessary.

<CloudPlatformToggle id="cloud-platform-toggle-dns">
<div slot="gcp">

In this reference architecture, we use [Google Cloud DNS](https://cloud.google.com/dns) for domain name resolution. To automatically configure Cloud DNS, we use [External DNS for Kubernetes](https://github.com/kubernetes-sigs/external-dns).

First, we need a **service account** with role `roles/dns.admin`. This service account is needed by cert-manager to alter the DNS settings for the DNS-01 resolution.

```bash
DNS_SA=gitpod-dns01-solver
DNS_SA_EMAIL="${DNS_SA}"@"${PROJECT_NAME}".iam.gserviceaccount.com
gcloud iam service-accounts create "${DNS_SA}" --display-name "${DNS_SA}"
gcloud projects add-iam-policy-binding "${PROJECT_NAME}" \
    --member serviceAccount:"${DNS_SA_EMAIL}" --role="roles/dns.admin"
```

Save the service account key to the file `./dns-credentials.json`:

```bash
gcloud iam service-accounts keys create --iam-account "${DNS_SA_EMAIL}" \
    ./dns-credentials.json
```

After that, we create a [managed zone](https://cloud.google.com/dns/docs/zones).

```bash
DOMAIN=gitpod.example.com
gcloud dns managed-zones create "${CLUSTER_NAME}" \
    --dns-name "${DOMAIN}." \
    --description "Automatically managed zone by kubernetes.io/external-dns"
```

Now we are ready to install External DNS. Please refer to the [External DNS GKE docs](https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/gke.md).

<details>
  <summary  class="text-p-medium">Example on how to install External DNS with helm</summary>

```bash
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
helm upgrade \
    --atomic \
    --cleanup-on-fail \
    --create-namespace \
    --install \
    --namespace external-dns \
    --reset-values \
    --set provider=google \
    --set google.project="${PROJECT_NAME}" \
    --set logFormat=json \
    --set google.serviceAccountSecretKey=dns-credentials.json \
    --wait \
    external-dns \
    bitnami/external-dns
```

</details>

Depending on what your DNS setup for your domain looks like, you most probably want to configure the nameservers for your domain. Run the following command to get a list of nameservers used by your Cloud DNS setup:

```bash
gcloud dns managed-zones describe ${CLUSTER_NAME} --format json | jq '.nameServers'
```

</div>
<div slot="aws">

If a Route53 zone has not yet been created, you can do so with the following command (replace `gitpod.example.com.` with the intended zone):

```bash
DOMAIN_NAME="gitpod.example.com"
export ROUTE53_CALLER=$(cat /proc/sys/kernel/random/uuid)
aws route53 create-hosted-zone \
    --name "${DOMAIN_NAME}." \
    --caller-reference $ROUTE53_CALLER \
    --hosted-zone-config Comment="gitpod-zone"
```

Once the domain has been provisioned, you can get the details with the following command and record the `Id` for later usage:

```bash
aws route53 list-hosted-zones --query "HostedZones[?Name==\`$DOMAIN_NAME.\`]"
```

Which should return something like:

```bash
[
    {
        "Id": "/hostedzone/Z1230498123094",
        "Name": "gitpod.example.com.",
        "CallerReference": "c43f5dfd-87d9-45fc-bb1f-33bf35d8244b",
        "Config": {
            "Comment": "",
            "PrivateZone": false
        },
        "ResourceRecordSetCount": 10
    }
]
```

Store this hosted zone in a variable for later use by external-dns and cert-manager:

```bash
HOSTED_ZONE_ID="$(aws route53 list-hosted-zones-by-name \
    --dns-name "$DOMAIN_NAME." \
    --query "HostedZones[0].Id" \
    --output json \
    --out text)"
```

Then install external-dns so that DNS records will be automatically created for Gitpod services. This can be ignored if you are managing DNS records yourself.

<!--
external-dns helm chart notes:

- `eksctl` is responsible for creating the external-dns Kubernetes service account and attaching
  an AWS IAM role to to the external-dns service account. The helm chart assumes that the service
  account has been pre-created (`--set serviceAccount.create=false`)
- For security purposes external-dns runs with a [UID of 65534](https://github.com/kubernetes-sigs/external-dns/blob/v0.12.2/Dockerfile#L35-L37)
- EKS IAM roles for service accounts expose AWS token files into the container; as external-dns
  is running as a non-root user the `fsGroup` setting must be set to change the ownership of the
  AWS token file to match the UID of the external-dns process.

See also: https://aws.amazon.com/premiumsupport/knowledge-center/eks-set-up-externaldns/
-->

```bash
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
helm upgrade \
    --atomic \
    --cleanup-on-fail \
    --create-namespace \
    --install \
    --namespace external-dns \
    --reset-values \
    --wait \
    --set provider=aws \
    --set aws.zoneType=public \
    --set txtOwnerId="$HOSTED_ZONE_ID" \
    --set "domainFilters[0]=$DOMAIN_NAME" \
    --set serviceAccount.create=false \
    --set serviceAccount.name=external-dns \
    --set podSecurityContext.fsGroup=65534 \
    external-dns \
    bitnami/external-dns
```

With Route53 created, you can now proceed to install cert-manager. Cert-manager is needed for Gitpod's internal networking even if you are managing DNS yourself.

</div>

<div slot="azure">

This section will create an Azure managed zone, grant the AKS cluster permission to manage records in that zone, and install external-dns.

Begin by creating a new Azure managed zone. For example, if you plan on hosting Gitpod at `gitpod.svcs.example.com` then create a managed zone called `svcs.example.com`.

```bash
DOMAIN_NAME="svcs.example.com"
az network dns zone create --name $DOMAIN_NAME --resource-group $RESOURCE_GROUP
```

Authorize the AKS cluster to control DNS records in the zone:

```bash
ZONE_ID=$(az network dns zone show --name "${DOMAIN_NAME}" --resource-group "${RESOURCE_GROUP}" --query "id" -o tsv)
KUBELET_OBJECT_ID=$(az aks show --name "${CLUSTER_NAME}" --resource-group "${RESOURCE_GROUP}" --query "identityProfile.kubeletidentity.objectId" -o tsv)

az role assignment create \
    --assignee "${KUBELET_OBJECT_ID}" \
    --role "DNS Zone Contributor" \
    --scope "${ZONE_ID}"
```

> This role assignment uses [AKS Kubelet Identity](https://cert-manager.io/docs/configuration/acme/dns01/azuredns/#managed-identity-using-aks-kubelet-identity)
> to authorizes the entire AKS cluster to manage DNS records in the given zone, including cert-manager and external-dns.

Look up the AKS kubelet client identity; external-dns will use this identity when authenticating to the Azure API.

```bash
KUBELET_CLIENT_ID=$(az aks show --name "${CLUSTER_NAME}" --resource-group "${RESOURCE_GROUP}" --query "identityProfile.kubeletidentity.clientId" -o tsv)
```

Then install the external-dns Helm chart:

```bash
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
helm upgrade \
    --install \
    --atomic \
    --wait \
    --cleanup-on-fail \
    --create-namespace \
    --namespace external-dns \
    --reset-values \
    --set provider=azure \
    --set azure.resourceGroup="${RESOURCE_GROUP}" \
    --set azure.subscriptionId="${AZURE_SUBSCRIPTION_ID}" \
    --set azure.tenantId="${AZURE_TENANT_ID}" \
    --set azure.useManagedIdentityExtension=true \
    --set azure.userAssignedIdentityID="${KUBELET_CLIENT_ID}" \
    --set logFormat=json \
    external-dns \
    bitnami/external-dns
```

</div>

</CloudPlatformToggle>

### cert-manager

Gitpod uses TLS secure external traffic bound for Gitpod as well as identifying, authorizing, and securing internal traffic between Gitpod's internal components. While you can provide your own TLS certificate for securing external connections to Gitpod, cert-manager is required to generate internal TLS certificates.

Refer to the [cert-manager DNS01 docs](https://cert-manager.io/docs/configuration/acme/dns01/) for more information.

<CloudPlatformToggle>
<div slot="gcp">

Example on how to install cert-manager on GCP:

```bash
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm upgrade \
    --atomic \
    --cleanup-on-fail \
    --create-namespace \
    --install \
    --namespace cert-manager \
    --reset-values \
    --set installCRDs=true \
    --set 'extraArgs={--dns01-recursive-nameservers-only=true,--dns01-recursive-nameservers=8.8.8.8:53\,1.1.1.1:53}' \
    --wait \
    cert-manager \
    jetstack/cert-manager
```

</div>
<div slot="aws">

Due to the networking behavior and service accounts in EKS, cert-manager needs a different installation procedure. First, install cert-manager with the following command:

```bash
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm upgrade \
    --atomic \
    --cleanup-on-fail \
    --create-namespace \
    --install \
    --namespace cert-manager \
    --reset-values \
    --set installCRDs=true \
    --set 'extraArgs={--dns01-recursive-nameservers-only=true,--dns01-recursive-nameservers=8.8.8.8:53\,1.1.1.1:53}' \
    --set webhook.hostNetwork=true \
    --set webhook.securePort=10260 \
    --set serviceAccount.create=false \
    --set serviceAccount.name=cert-manager \
    --wait \
    cert-manager \
    jetstack/cert-manager
```

Once the installation has completed, you will need to update the cert-manager security context setting for the service account provisioned for cert-manager by `eksctl`:

```bash
kubectl patch deployment cert-manager -n cert-manager -p \
  '{"spec":{"template":{"spec":{"securityContext":{"fsGroup":1001,"runAsNonRoot": true}}}}}'

```

</div>

<div slot="azure">

Install cert-manager with the following command:

```bash
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm upgrade \
    --install \
    --atomic \
    --wait \
    --cleanup-on-fail \
    --create-namespace \
    --namespace='cert-manager' \
    --reset-values \
    --set installCRDs=true \
    --set 'extraArgs={--dns01-recursive-nameservers-only=true,--dns01-recursive-nameservers=8.8.8.8:53\,1.1.1.1:53}' \
    cert-manager \
    jetstack/cert-manager
```

</div>
</CloudPlatformToggle>

### TLS certificate

In this reference architecture, we use cert-manager to also create **TLS certificates for the Gitpod domain**. Since we need wildcard certificates for the subdomains, you must use the [DNS-01 challenge](https://letsencrypt.org/docs/challenge-types/#dns-01-challenge).

Using a certificate issued by Let's Encrypt is recommended as it minimizes overhead involving TLS certificates and managing CA certificate trust, but is not required. If you already have TLS certificates for your Gitpod installation with suitable DNS names you can skip this step and use your own certificates during the installation.

<CloudPlatformToggle id="cloud-platform-toggle-cert-manager-tls">
<div slot="gcp">

Now, we are configuring [Google Cloud DNS for the DNS-01 challenge](https://cert-manager.io/docs/configuration/acme/dns01/google/). For this, we need to create a secret that contains the key for the DNS service account:

```bash
CLOUD_DNS_SECRET=clouddns-dns01-solver
kubectl create secret generic "${CLOUD_DNS_SECRET}" \
    --namespace=cert-manager \
    --from-file=key.json="./dns-credentials.json"
```

After that, we are telling cert-manager which service account it should use:

```bash
kubectl annotate serviceaccount --namespace=cert-manager cert-manager \
    --overwrite "iam.gke.io/gcp-service-account=${DNS_SA_EMAIL}"
```

The next step is to create an issuer. In this guide, we create a cluster issuer. Create a file `issuer.yaml` like this:

```yaml
# Replace $LETSENCRYPT_EMAIL with your email and $PROJECT_NAME with your GCP project name
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
    name: gitpod-issuer
spec:
    acme:
        email: $LETSENCRYPT_EMAIL
        server: https://acme-v02.api.letsencrypt.org/directory
        privateKeySecretRef:
            name: issuer-account-key
        solvers:
            - dns01:
                  cloudDNS:
                      project: $PROJECT_NAME
```

… and run:

```bash
kubectl apply -f issuer.yaml
```

</div>
<div slot="aws">

If using `eksctl` and the cert-manager service account along with well-known policies AND you have your intended zone hosted in Route53, then follow the [cert-manager](https://cert-manager.io/docs/configuration/acme/dns01/route53/) configuration steps. An example cluster issuer using the hosted zone and cert-manager service account created by `eksctl` is below:

```yaml
# Replace $LETSENCRYPT_EMAIL with your email and $DOMAIN_NAME with your gitpod domain name (eg.`gitpod.$DOMAIN_NAME`)
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
    name: gitpod-issuer
spec:
    acme:
        server: https://acme-v02.api.letsencrypt.org/directory
        email: $LETSENCRYPT_EMAIL
        privateKeySecretRef:
            name: letsencrypt
        solvers:
            - selector:
                  dnsZones:
                      - $DOMAIN_NAME
              dns01:
                  route53:
                      region: us-east-1
```

> ⚠️ In contrast to most AWS services, Route53 does _not_ support regional endpoints. When creating your ClusterIssuer
> be careful to use the `us-east-1` region for all regions other than the Beijing and Ningxia Regions, and `cn-northwest-1`
> region for the Beijing and Ningxia regions.
>
> See the [AWS Route53 endpoints and quotas documentation](https://docs.aws.amazon.com/general/latest/gr/r53.html) for more information.

</div>

<div slot="azure">

This section will create a cert-manager ClusterIssuer that will generate publicly trusted certificates using Let's Encrypt.

First, determine your Azure subscription ID. You can typically determine your subscription ID from your Azure CLI credentials.

```bash
AZURE_SUBSCRIPTION_ID="$(az account subscription list --query '[0].subscriptionId' --output tsv)"
```

Then create a file named `issuer.yaml` containing the following content, expanding the `$AZURE_SUBSCRIPTION_ID`, `$RESOURCE_GROUP`, and `$DOMAIN_NAME` variables:

```yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
    name: gitpod-issuer
spec:
    acme:
        server: https://acme-v02.api.letsencrypt.org/directory
        privateKeySecretRef:
            name: issuer-account-key
        solvers:
            - dns01:
                  azureDNS:
                      subscriptionID: $AZURE_SUBSCRIPTION_ID
                      resourceGroupName: $RESOURCE_GROUP
                      hostedZoneName: $DOMAIN_NAME
```

Then apply the ClusterIssuer resource:

```bash
kubectl apply -f issuer.yaml
```

> This example ClusterIssuer depends on [Azure Managed Identity](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) to authorize requests from cert-manager to the AzureDNS API.
> Refer to the [cert-manager AzureDNS DNS01](https://cert-manager.io/docs/configuration/acme/dns01/azuredns/) documentation for more information on cert-manager API authorization.

</CloudPlatformToggle>
